x_train = predict(dummies, newdata = x_train)
x_test = predict(dummies, newdata = x_test)
x_train = as.data.frame(x_train)
x_test = as.data.frame(x_test)
x_train$M_Trip_mode_4 = df_train$M_Trip_mode_4
x_test$M_Trip_mode_4 = df_test$M_Trip_mode_4
hyper_grid <- expand.grid(
size  = c(10,15,20),
decay = c(1e-4,5e-4,1e-3),
MWE = c(1,2)
)
# create hyperparameter grid
hyper_grid <- expand.grid(
eta = c(.05, .1, .3),
max_depth = c(3, 5, 7),
min_child_weight = c(3, 5, 7),
subsample = c( .8, 1),
colsample_bytree = c(.8, 1),
optimal_trees = 0,               # a place to dump results
min_RMSE = 0                     # a place to dump results
)
## tuned parameters
params <- list(
eta = 0.3,
max_depth = 11,
min_child_weight = 5,
subsample = 0.8,
colsample_bytree = 1
)
# create hyperparameter grid
hyper_grid <- expand.grid(
eta = c(.05, .1, .3),
max_depth = c(3, 5, 7),
min_child_weight = c(3, 5, 7),
subsample = c( .8, 1),
colsample_bytree = c(.8, 1),
optimal_trees = 0,               # a place to dump results
min_RMSE = 0                     # a place to dump results
)
## The tuned parameters
params <- list(
eta = 0.3,
max_depth = 11,
min_child_weight = 5,
subsample = 0.8,
colsample_bytree = 1
)
## Training the XGB using tuned parameters
xgb.tune <- xgboost(
params = params,
data = data.matrix(subset(df_train, select = -M_Trip_mode_4)),
label = as.vector(df_train$M_Trip_mode_4),
nrounds = 500,
nfold = 4,
objective = "multi:softprob",  # for classification
verbose = 1,               # silent,
early_stopping_rounds = 50, # stop if no improvement for 10 consecutive trees
num_class = 4,
weight = model_weights*100000
)
pred <- predict(xgb.tune,data.matrix(subset(df_test, select = -M_Trip_mode_4)))
pred = matrix(pred,ncol=4,byrow=TRUE)
pred_labels = max.col(pred)-1
value_xgb = factor(pred_labels,levels = levels(df_test$M_Trip_mode_4))
caret::confusionMatrix(data = value_xgb,reference = df_test$M_Trip_mode_4)
sum(model_weights)
caret::confusionMatrix(data = value_rf,reference = df_test$M_Trip_mode_4)
pred_xgb_prob = function(object, newdata)  {
pred <- predict(object,data.matrix(subset(newdata, select = -M_Trip_mode_4)))
pred = matrix(pred,ncol=4,byrow=TRUE)
return(pred)
}
pred_xgb = function(object, newdata)  {
pred <- predict(object,data.matrix(subset(newdata, select = -M_Trip_mode_4)))
pred = matrix(pred,ncol=4,byrow=TRUE)
pred_labels = max.col(pred)-1
pred = factor(pred_labels,levels = levels(df_test$M_Trip_mode_4))
return(pred)
}
pred_xgb(xgb.tune,df_test)
pred_xgb_prob(xgb.tune,df_test)
### Feature importance
## Define Loss function for calculating feature importance
Bal_AU <- function(actual, predicted) {
caret::confusionMatrix(predicted,actual)$byClass[1,11]
}
Bal_BI <- function(actual, predicted) {
caret::confusionMatrix(predicted,actual)$byClass[2,11]
}
Bal_PT <- function(actual, predicted) {
caret::confusionMatrix(predicted,actual)$byClass[3,11]
}
Bal_WL <- function(actual, predicted) {
caret::confusionMatrix(predicted,actual)$byClass[4,11]
}
Bal_All <- function(actual, predicted) {
mean(caret::confusionMatrix(predicted,actual)$byClass[,11])
}
df_test_x = subset(df_test, select = -M_Trip_mode_4)
Predictor_xgb_prob = iml::Predictor$new(
model = xgb.tune,
data = df_test_x,
y = df_test$M_Trip_mode_4,
predict.fun = pred_xgb_prob_int,
type="prob")
df_test_x
Predictor_xgb_prob = iml::Predictor$new(
model = xgb.tune,
data = df_test_x,
y = df_test$M_Trip_mode_4,
predict.fun = pred_xgb_prob_int,
type="prob")
pred_xgb_prob_int = function(object, newdata)  {
pred <- predict(object,data.matrix(newdata))
pred = matrix(pred,ncol=4,byrow=TRUE)
return(pred)
}
Predictor_xgb_prob = iml::Predictor$new(
model = xgb.tune,
data = df_test_x,
y = df_test$M_Trip_mode_4,
predict.fun = pred_xgb_prob_int,
type="prob")
Predictor_xgb_prob
st = Sys.time()
xgb_ale_prob_100 = iml::FeatureEffects$new(Predictor_xgb_prob,method = "ale", grid.size = 100)
xgb_ale_prob_200 = iml::FeatureEffects$new(Predictor_xgb_prob,method = "ale", grid.size = 200)
et = Sys.time()
et-st
pal <- wes_palette("Zissou1", 100, type = "continuous")
st = Sys.time()
Predictor_xgb_prob$data$
xlim_1 = quantile(df_test_x$Age, prob = c(0,0.9))[1]
xlim_2 = quantile(df_test_x$Age, prob = c(0,0.9))[2]
ylim_1 = quantile(df_test_x$Travel_time, prob = c(0,0.9))[1]
ylim_2 = quantile(df_test_x$Travel_time, prob = c(0,0.9))[2]
xgb_ale_prob_2d = FeatureEffect$new(Predictor_xgb_prob, c("Age", "Travel_time"), grid.size = 100)
xgb_ale_prob_2d$results = xgb_ale_prob_2d$results[ xgb_ale_prob_2d$results$Age > xlim_1 &
xgb_ale_prob_2d$results$Age < xlim_2 &
xgb_ale_prob_2d$results$Travel_time > ylim_1 &
xgb_ale_prob_2d$results$Travel_time < ylim_2, ]
xgb_ale_prob_100$plot(show.data=FALSE)
xgb_ale_prob_100$plot
xgb_ale_prob_100$plot(show.data=FALSE)
library(patchwork)
install.packages('patchwork')
library(patchwork)
xgb_ale_prob_100$plot(show.data=FALSE)
load("Results/XGB_ALE_Class1.Rdata")
XGB_ALE_Class1 = load("Results/XGB_ALE_Class1.Rdata")
load("Results/XGB_ALE_Class1.Rdata")
plot(xgb_int_prob_0)
load("Results/XGB_Int_Class1.Rdata")
plot(xgb_int_prob_0)
plot(xgb_int_prob_1)
load("Results/XGB_Int_Class4.Rdata")
plot(xgb_int_prob_1)
load("Results/XGB_Int_Class2.Rdata")
plot(xgb_int_prob_2)
load("Results/XGB_Int_Class3.Rdata")
plot(xgb_int_prob_3)
load("Results/XGB_Int_Class1.Rdata")
plot(xgb_int_prob_0)
load("Results/XGB_Int_Class2.Rdata")
plot(xgb_int_prob_1)
load("Results/XGB_Int_Class3.Rdata")
plot(xgb_int_prob_2)
load("Results/XGB_Int_Class4.Rdata")
plot(xgb_int_prob_3)
xgb_ale_prob_100
plot(xgb_ale_prob_100,features = c("Age","Car_owener","Number_of_trips","Sum_of_travel_time","Travel_time","Activity_duration"),fixed_y = FALSE)
xgb_ale_prob_100
xlim_1 = quantile(df_test_x$Age, prob = c(0,0.9))[1]
xlim_2 = quantile(df_test_x$Age, prob = c(0,0.9))[2]
ylim_1 = quantile(df_test_x$Travel_time, prob = c(0,0.9))[1]
ylim_2 = quantile(df_test_x$Travel_time, prob = c(0,0.9))[2]
xgb_ale_prob_2d = FeatureEffect$new(Predictor_xgb_prob, c("Age", "Travel_time"), grid.size = 100)
install.packages("yaImpute")
library(yaImpute)
xlim_1 = quantile(df_test_x$Age, prob = c(0,0.9))[1]
xlim_2 = quantile(df_test_x$Age, prob = c(0,0.9))[2]
ylim_1 = quantile(df_test_x$Travel_time, prob = c(0,0.9))[1]
ylim_2 = quantile(df_test_x$Travel_time, prob = c(0,0.9))[2]
xgb_ale_prob_2d = FeatureEffect$new(Predictor_xgb_prob, c("Age", "Travel_time"), grid.size = 100)
xgb_ale_prob_2d$results = xgb_ale_prob_2d$results[ xgb_ale_prob_2d$results$Age > xlim_1 &
xgb_ale_prob_2d$results$Age < xlim_2 &
xgb_ale_prob_2d$results$Travel_time > ylim_1 &
xgb_ale_prob_2d$results$Travel_time < ylim_2, ]
xgb_ale_prob_2d$plot(show.data = FALSE) +
scale_y_continuous("Travel_time")  +
scale_x_continuous("Age") +
scale_fill_gradientn(colours = pal) +
ylim(ylim_1,ylim_2) +
xlim(xlim_1,xlim_2)
st = Sys.time()
xgb_imp_full = vip::vip(xgb.tune,
train = df_test,
target = df_test$M_Trip_mode_4,
metric = Bal_All,
method = "permute",
nsim = 50,
sample_frac = 0.5,
pred_wrapper = pred_xgb,
smaller_is_better = FALSE,
geom = "boxplot",
all_permutations = TRUE,
mapping = aes_string(fill = "Variable"),
aesthetics = list(color = "grey35")) +
ggtitle("XGB_IMP_FULL")
et = Sys.time()
et-st
plot(xgb_imp_full)
load("XGB_Imp.Rdata")
load("Results/XGB_Imp.Rdata")
plot(xgb_imp_full)
---
title: "IML_ModeChoice"
author: "Eui-Jin Kim"
date: 'Aug 10 2021'
output: html_document
editor_options:
chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Data Preprocessing
This code provide sampled travel survey data collected in Seoul metropolitan area in 2016.
The data is processed to use for Machine Learning (ML) and Interpretable ML (IML) models.
Detailed data descrptions are provided in the paper https://doi.org/10.1155/2021/6685004
#### Load the library and parallel processing setting
```{r,warning=FALSE,message=FALSE}
library(data.table)
library(plyr)
library(dplyr)
library(caret)
library(parallel)
library(doParallel)
library(foreach)
library(iml)
library(rsample)
library(vip)
library(xgboost)
library(viridis)
library(wesanderson)
library(ranger)
library(patchwork)
library(yaImpute)
set.seed(123)
numCores = detectCores()-2
cl = makeCluster(numCores)
registerDoParallel(cl)
```
#### Data preparation
```{r,warning=FALSE}
df = fread("data_travelMode_2016_sampled.csv",sep=',',header = TRUE,fill=TRUE)
## Remains only relevant columns
df_model = df[,!(c("ID","Home_code","P_Depart_code","P_Arrival_code",
"P_Trip_purpose","Home_type","P_Trip_seq","P_Home_Access",
"start_time","P_Arr_LU3","P_Arr_Meanage","P_Arr_Older","Last_trip"))]
## Transform the data types
df_model$Home_car = factor(df_model$Home_car)
df_model$Home_income = factor(df_model$Home_income)
df_model$Home_drive = factor(df_model$Home_drive)
df_model$Gender = factor(df_model$Gender)
df_model$Max_seq = factor(df_model$Max_seq)
df_model$P_Depart_time = factor(df_model$P_Depart_time)
df_model$M_Trip_mode_4 = factor(df_model$M_Trip_mode_4)
df_model$P_Tour_type = factor(df_model$P_Tour_type)
df_model$P_Trip_type = factor(df_model$P_Trip_type)
# Remove the Minor class Taxi
df_model = df_model[df_model$M_Trip_mode_4 != "Taxi",]
df_model$M_Trip_mode_4 = factor(df_model$M_Trip_mode_4, levels = c("Auto","Bike","PT","Walk"), labels = c(0,1,2,3))
colnames(df_model) = c("Income","Car_owener","Driver_license","Age","Gender","Number_of_trips",
"Departure_time","Travel_time","Land_use_in_D_Residential","Land_use_in_D_Commerical",
"Activity_duration","Population_density_at_D","Number_of_workers_at_D","Number_of_bus_stops_at_D","Number_of_subway_stops_at_D","M_Trip_mode_4",
"Trip_type","Sum_of_travel_time","Sum_of_activity_duration","Tour_type")
# Split the data into training and test-set
set.seed(123)
df_split <- initial_split(df_model, prop = .75)
df_train <- training(df_split)
df_test  <- testing(df_split)
features <- setdiff(names(df_train), "M_Trip_mode_4")
```
---
title: "IML_ModeChoice"
author: "Eui-Jin Kim"
date: 'Aug 10 2021'
output: html_document
editor_options:
chunk_output_type: console
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Data Preprocessing
This code provide sampled travel survey data collected in Seoul metropolitan area in 2016.
The data is processed to use for Machine Learning (ML) and Interpretable ML (IML) models.
Detailed data descrptions are provided in the paper https://doi.org/10.1155/2021/6685004
#### Load the library and parallel processing setting
```{r,warning=FALSE,message=FALSE}
library(data.table)
library(plyr)
library(dplyr)
library(caret)
library(parallel)
library(doParallel)
library(foreach)
library(iml)
library(rsample)
library(vip)
library(xgboost)
library(viridis)
library(wesanderson)
library(ranger)
library(patchwork)
library(yaImpute)
set.seed(123)
numCores = detectCores()-2
cl = makeCluster(numCores)
registerDoParallel(cl)
```
#### Data preparation
```{r,warning=FALSE}
df = fread("Data/data_travelMode_2016_sampled.csv",sep=',',header = TRUE,fill=TRUE)
## Remains only relevant columns
df_model = df[,!(c("ID","Home_code","P_Depart_code","P_Arrival_code",
"P_Trip_purpose","Home_type","P_Trip_seq","P_Home_Access",
"start_time","P_Arr_LU3","P_Arr_Meanage","P_Arr_Older","Last_trip"))]
## Transform the data types
df_model$Home_car = factor(df_model$Home_car)
df_model$Home_income = factor(df_model$Home_income)
df_model$Home_drive = factor(df_model$Home_drive)
df_model$Gender = factor(df_model$Gender)
df_model$Max_seq = factor(df_model$Max_seq)
df_model$P_Depart_time = factor(df_model$P_Depart_time)
df_model$M_Trip_mode_4 = factor(df_model$M_Trip_mode_4)
df_model$P_Tour_type = factor(df_model$P_Tour_type)
df_model$P_Trip_type = factor(df_model$P_Trip_type)
# Remove the Minor class Taxi
df_model = df_model[df_model$M_Trip_mode_4 != "Taxi",]
df_model$M_Trip_mode_4 = factor(df_model$M_Trip_mode_4, levels = c("Auto","Bike","PT","Walk"), labels = c(0,1,2,3))
colnames(df_model) = c("Income","Car_owener","Driver_license","Age","Gender","Number_of_trips",
"Departure_time","Travel_time","Land_use_in_D_Residential","Land_use_in_D_Commerical",
"Activity_duration","Population_density_at_D","Number_of_workers_at_D","Number_of_bus_stops_at_D","Number_of_subway_stops_at_D","M_Trip_mode_4",
"Trip_type","Sum_of_travel_time","Sum_of_activity_duration","Tour_type")
# Split the data into training and test-set
set.seed(123)
df_split <- initial_split(df_model, prop = .75)
df_train <- training(df_split)
df_test  <- testing(df_split)
features <- setdiff(names(df_train), "M_Trip_mode_4")
```
df = fread("Data/data_travelMode_2016_sampled.csv",sep=',',header = TRUE,fill=TRUE)
## Remains only relevant columns
df_model = df[,!(c("ID","Home_code","P_Depart_code","P_Arrival_code",
"P_Trip_purpose","Home_type","P_Trip_seq","P_Home_Access",
"start_time","P_Arr_LU3","P_Arr_Meanage","P_Arr_Older","Last_trip"))]
## Transform the data types
df_model$Home_car = factor(df_model$Home_car)
df_model$Home_income = factor(df_model$Home_income)
df_model$Home_drive = factor(df_model$Home_drive)
df_model$Gender = factor(df_model$Gender)
df_model$Max_seq = factor(df_model$Max_seq)
df_model$P_Depart_time = factor(df_model$P_Depart_time)
df_model$M_Trip_mode_4 = factor(df_model$M_Trip_mode_4)
df_model$P_Tour_type = factor(df_model$P_Tour_type)
df_model$P_Trip_type = factor(df_model$P_Trip_type)
# Remove the Minor class Taxi
df_model = df_model[df_model$M_Trip_mode_4 != "Taxi",]
df_model$M_Trip_mode_4 = factor(df_model$M_Trip_mode_4, levels = c("Auto","Bike","PT","Walk"), labels = c(0,1,2,3))
colnames(df_model) = c("Income","Car_owener","Driver_license","Age","Gender","Number_of_trips",
"Departure_time","Travel_time","Land_use_in_D_Residential","Land_use_in_D_Commerical",
"Activity_duration","Population_density_at_D","Number_of_workers_at_D","Number_of_bus_stops_at_D","Number_of_subway_stops_at_D","M_Trip_mode_4",
"Trip_type","Sum_of_travel_time","Sum_of_activity_duration","Tour_type")
# Split the data into training and test-set
set.seed(123)
df_split <- initial_split(df_model, prop = .75)
df_train <- training(df_split)
df_test  <- testing(df_split)
features <- setdiff(names(df_train), "M_Trip_mode_4")
warnings()
## Hyper-parameter grids for XGboost
hyper_grid <- expand.grid(
eta = c(.05, .1, .3),
max_depth = c(3, 5, 7),
min_child_weight = c(3, 5, 7),
subsample = c( .8, 1),
colsample_bytree = c(.8, 1),
optimal_trees = 0,               # a place to dump results
min_RMSE = 0                     # a place to dump results
)
## Load the tuned parameters (tuned from the server)
params <- list(
eta = 0.3,
max_depth = 11,
min_child_weight = 5,
subsample = 0.8,
colsample_bytree = 1
)
## Training the tuned XGB models
xgb.tune <- xgboost(
params = params,
data = data.matrix(subset(df_train, select = -M_Trip_mode_4)),
label = as.vector(df_train$M_Trip_mode_4),
nrounds = 500,
nfold = 4,
objective = "multi:softprob",  # for classification
verbose = 1,               # silent,
early_stopping_rounds = 50, # stop if no improvement for 10 consecutive trees
num_class = 4,
weight = model_weights*100000
)
# Define XGB prediction function
pred_xgb_prob_int = function(object, newdata)  {
pred <- predict(object,data.matrix(newdata))
pred = matrix(pred,ncol=4,byrow=TRUE)
return(pred)
}
## Measuring classification performance using confusion matrix
pred <- predict(xgb.tune,data.matrix(subset(df_test, select = -M_Trip_mode_4)))
pred = matrix(pred,ncol=4,byrow=TRUE)
pred_labels = max.col(pred)-1
value_xgb = factor(pred_labels,levels = levels(df_test$M_Trip_mode_4))
caret::confusionMatrix(data = value_xgb,reference = df_test$M_Trip_mode_4)
## Hyper-parameter grids for XGboost
hyper_grid <- expand.grid(
eta = c(.05, .1, .3),
max_depth = c(3, 5, 7),
min_child_weight = c(3, 5, 7),
subsample = c( .8, 1),
colsample_bytree = c(.8, 1),
optimal_trees = 0,               # a place to dump results
min_RMSE = 0                     # a place to dump results
)
## Load the tuned parameters (tuned from the server)
params <- list(
eta = 0.3,
max_depth = 11,
min_child_weight = 5,
subsample = 0.8,
colsample_bytree = 1
)
## Training the tuned XGB models
xgb.tune <- xgboost(
params = params,
data = data.matrix(subset(df_train, select = -M_Trip_mode_4)),
label = as.vector(df_train$M_Trip_mode_4),
nrounds = 500,
nfold = 4,
objective = "multi:softprob",  # for classification
verbose = 1,               # silent,
early_stopping_rounds = 50, # stop if no improvement for 10 consecutive trees
num_class = 4,
weight = model_weights*100000,
verbose = FALSE
)
# Define XGB prediction function
pred_xgb_prob_int = function(object, newdata)  {
pred <- predict(object,data.matrix(newdata))
pred = matrix(pred,ncol=4,byrow=TRUE)
return(pred)
}
## Measuring classification performance using confusion matrix
pred <- predict(xgb.tune,data.matrix(subset(df_test, select = -M_Trip_mode_4)))
pred = matrix(pred,ncol=4,byrow=TRUE)
pred_labels = max.col(pred)-1
value_xgb = factor(pred_labels,levels = levels(df_test$M_Trip_mode_4))
caret::confusionMatrix(data = value_xgb,reference = df_test$M_Trip_mode_4)
## Hyper-parameter grids for XGboost
hyper_grid <- expand.grid(
eta = c(.05, .1, .3),
max_depth = c(3, 5, 7),
min_child_weight = c(3, 5, 7),
subsample = c( .8, 1),
colsample_bytree = c(.8, 1),
optimal_trees = 0,               # a place to dump results
min_RMSE = 0                     # a place to dump results
)
## Load the tuned parameters (tuned from the server)
params <- list(
eta = 0.3,
max_depth = 11,
min_child_weight = 5,
subsample = 0.8,
colsample_bytree = 1
)
## Training the tuned XGB models
xgb.tune <- xgboost(
params = params,
data = data.matrix(subset(df_train, select = -M_Trip_mode_4)),
label = as.vector(df_train$M_Trip_mode_4),
nrounds = 500,
nfold = 4,
objective = "multi:softprob",  # for classification
verbose = 1,               # silent,
early_stopping_rounds = 50, # stop if no improvement for 10 consecutive trees
num_class = 4,
weight = model_weights*100000
)
# Define XGB prediction function
pred_xgb_prob_int = function(object, newdata)  {
pred <- predict(object,data.matrix(newdata))
pred = matrix(pred,ncol=4,byrow=TRUE)
return(pred)
}
## Measuring classification performance using confusion matrix
pred <- predict(xgb.tune,data.matrix(subset(df_test, select = -M_Trip_mode_4)))
pred = matrix(pred,ncol=4,byrow=TRUE)
pred_labels = max.col(pred)-1
value_xgb = factor(pred_labels,levels = levels(df_test$M_Trip_mode_4))
caret::confusionMatrix(data = value_xgb,reference = df_test$M_Trip_mode_4)
